DONE

get basic carla setup working again on matrix
get object-centric forecast tfrs

get r2p2 forecasting to work again
> note i need the object-centric version
> note it never has worked before in pytorch; this is a new install
> install a prinet and train it a bit
>> get an object-centric crop, like you made for the other guy
>> i think what you need to do is:
>>> assume egomotion is resolved
>>> use R0 coordinates
>>> get the trajectory in those coords, and vis it
>>> train prinet with the object-centric data
>>> give prinet seg-invar convs, to make its output more predictable
>>> install the other components of rponet
>>> train the full thing and look at it
>>> add the expm_softclip and see if it improves results < nope

get raquel's thing working for real
> get some clothoid curves and vis them
> add a new motioncost net for this
> flatten the occs and featurize them individually first
> featurize the first T_past occs a bit
> debug clothoids offline (in a separate script)
> get clothoids to work online (within motioncost net)
> get the cost maps training
> vis best/worst

actual tasks:
> resurrect flow mode
> get data for it
> debug the data bugs
> clean up the spurious code/vis
> get the nets going
> get a small trainval gap

get the tracker happening
> set up a track mode using the same flow data;
> supervise a tiny net to take gathered object flows and convert this to an Rt
>> note this should happen in the test setup, where we are doing long-jump tracking (with frame0 template)
>> let's call this "reloc" mode
>> first make this using basic flow-to-rt; basically copy from track mode
>>> make the reloc mode
>>> make the reloc mode shell
>>> add in forecast data parsing, since that's my latest and cleanest
>>> port in the tracking code, to get some outputs happening
>>> note the data i am using here is considerably harder than what i used for the paper, since i am including tiny objects
>>>> generate a new subset/dataset where you ONLY include cars, and evaluate on this, to see if acc jumps
>>>> try evaluating the tracking in X0, to see if results improve over R0
>>> encapsulate the tracker, to help clean up the vars

the data appears to be a mess, and i don't understand/trust the loader anymore; let's take ownership and redo that
> fixed

evaluate the visibility thing, to see if there is enough, or if i need to generate it myself
> generate a new mode
> count up the number of lidar points inside the box, and make a plot of this
> ok indeed sometimes a thing gets occluded
> when <50 pts avail, it's pretty occluded i'd say
> it wouldn't be too difficult to create a set in which on frame7 there are <50 pts in the box

flow-tracker summary notes from past self:
> for some reason my supervised flownet is producing worse tracking results than the old unsupervised flownet (from cycles)
> i checked the L1 error of both, and the supervised one has lower error
> on the first step the supervised flownet does OK, but in longer trajectories he totally loses the object
> maybe the problem is that i'm always supervising the flow with adjacent frames
> so i need to retrain my sup flownet, and/or get unsup training to work
> i would like to know on-the-fly how good these flows are
>> this means: every N iterations, run the tracking eval
>> that way i know if i am improving things
concretely:
> get flo mode to run again
> get a fast/debug version going
> get/find S10 data
> install a custom altset thing
>> set up double_inputs, to not be tied to the old input style

high question:
should i pre-warp the inputs to featnet, or warp the features?
probably pre-warping inputs is a bit cleaner
doubling is an issue with the backwarps, which the real thing never needs to contend with
ah, but that's not the case if i do the rigid backwarps
so, how about: rigid-backwarp as early as possible -- maybe don't even do it as a warp; instead, just voxelize right there
ah and also: it is critical to show and train the model with some big egomotions, because otherwise it will behave crazily at track time

immediate next steps:
get the rigid backwarp happening
evaluate the tracker a bit
improve results by making the train/test gap small < ok good
add 50/50 stab/nonstab training and compare < not that much better at first, but let's see
get the rigid revox happening, and compare < ok, going well; let's see
vis the tracklets
< done
try an alt strategy where you do NOT chain flows but estimate the drifting way; overfitted flow should do well here
> skip this

set up a siamese tracker (to have something easier to supervise and improve, i.e., simpler)
> set up siamese mode
> voxelize near object
> add basic matchnet, which does a corr
> get label spheres in 3d
> set up loss
> weight the negs a bit higher, so that the bkg is indeed weighed low
> fix the issue of centroids sometimes being OOB, due to bounds being too small
> fix the issue of the loss being suspicious; copy from a good repo
> fix the (lack of) group conv; copy from a good repo
> clean up matchnet
> clean up the train block
> get vis of long seq tracking
> get eval of long seq tracking

get pairwise data that is not always sequential frames
> this can be done by editing the data loader a bit, to use traj stuff instead of seq
>> do this in the loader
>> fix up the model so that it works again

find out why performance with the rand-frame strategy is worse than it was with consec (flow) frames
> it is interesting that the test-time heamaps become black. is there an extreme value in here somewhere?
>> check the histogram and print stats
>>> ok, these are similar in train/test
>> normalize the corr maps per-frame
>>> good, this fixed the black
> the other potential issue is just a lack of training data
>> but this does not make a whole lot of sense, since we are right now training and testing on the same data, supposedly
> try making the trainval gap almost exactly zero
> the failure seems to come from the fact that voxels near the top edges get high corr scores
>> those voxels have no info in them!
>> masking them out from the corr is not super easy right now, due to the uneven-crop/resolution issues
>> adding info to them, by switching to sparse-sensitive convs, should resolve the fitting issue
>>> yes 

consider that the nonconsec sampling strat may give mostly pairs that are closeby
> ok let's ignore this

consider the issue of dot prod magnitude increasing with bigger crops
> do the 2d papers normalize in some other way?
>> no

check if the 2d methods ever update their template
> no, they certainly do not

get a search region happening
> done

find out if the search region strat improves perf as it should
> not really, but maybe my search region is too large

add cosine windowing
> done; this does improve perf quantitatively at the start

fix the (hypothesized) overfitting issue causing search-region-tracking to gradually do worse than one-shot
> try creating semi-random search regions at training time
>> this seems to have worked

make the search regions a little bit random, to improve generalization
> this helps a bit 

use higher res
> this helps a lot

get detnet happening in this same branch

split model_carla_det data_prep from model

make static mode work easily (for paul and niles)


DOING

get a reasonable retrieval metric happening (again) in static mode





SOON

see what happens if you make det use X0 instead of R

find out why higher res slows things down (considering only search regions are being featurized)

figure out something smart to do with dims/padding/downsampling, so that you end up with an odd-valued template size

get rotations happening

figure out how much we can/should downsample. probably 1/4 is better than 1/2 here.

get an unprojector that can work for the small crop/zoom region

do some profiling and speed it up

properly evaluate if flipping hurts timing or helps perf

categ the data as predictable-from-first-velocity and not, and use 50/50 samples

train a featnet concurrently, and warp its outputs into NY0 coords, and feed that to motioncost net


LATER

get the object traj parsing to work for BOTH carla and intphys






HIGH-LEVEL GOAL:

implement the thing we drew on the board in december (and probably also a year before that):
for t in range(1,T):
    if vis(l_fore):
       l = l_track
       l_fore = fore()
    else:
       l = l_fore


so we are saying:
> single object
> assume you have tracked it a bit first, so that you are able to forecast
>> no problem, let's just take the first 3 frames as given
> assume there is occasionally a visibility issue
>> i need to check if this is true
> maintain a "location" tensor, along with a "future traj" (l_fore) tensor as backup
>> ok
