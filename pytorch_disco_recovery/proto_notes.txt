

make a model which, at the last layer, classifies a fat voxel as a category of prototype
learn a dictionary of these prototypes
prototypes can be different sizes
prototypes need different poses
prototypes should be maybe 2x2x2 voxels large, so that they can overlap and connect





make a fully convolutional implicit model
at every location, produce an MLP which, when queried any number of times, gives us occupancies at floating-point locations nearby
this should be coarse-to-fine also.




consume a small 3d volume
output a vector
quantize this to the nearest neural net
note that this is like one-step routing
but i learned in vqvae that quantization does not help the latent space itself
this could be helpful if somehow i coudl learn the dictionary separately


starting at 16x16x16, output one mlp indicating the rough scene content
then, at each area where i want to know more, output a new mlp, conditioned on the content and raw data provided at the coarse level
i think the mlp generator can be shared across scales


to debug,
i can first try generating such an MLP for a single scene, at a single scale
this is basically NERF but with some gt occupancy and fewer views


another thing to think about is:
all these mini nets are going to be learning non-semantic shape bases
if i did something to allow rotation-aware translation-aware placement of the bases, then they should become semantically meaningful



some kind of routing process would be good to have too, so that bases compete for who will represent a part of space
that's instead of one-shot detection of spatial element


this idea doesn't necessarily need to be tied to implicit representations
it could be explicit voxel prototypes, of various sizes and shapes


the important thing is that we output a very high-resolution space, thanks to this voxel part composition

yes. it's just swapping a library of MLPs for a library of explicit 3d prototypes.

so how about some detector
rpn-style, or centernet-style
output the centroids and rotation parameters (let's say just yaw) of prototypes from the dictionary
then we paste everyone into place, according to the weights
scaling would be nice to have also
learn the prototypes at the same time
let the prototypes be huge: 4x128x128x128, for rgb-alpha
some scaling parameter should do trilinear scaling
spatial transformer

suppose we generate a voxelgrid shaped 4x256x256x256 once everyone is in place
the loss can be at this high resolution
the important thing is just that we do not attempt convolutions at crazy resolutions


we can take input at 256x256x256, and through convolutions, arrive at maybe 32x32x32
and we place at most that many entities, at subvoxel coordinates
we just sum up, capping the occupancy and using an occ-weighted mean of rgb

one issue with hypernets of implicit functions is that the implicit functions are apparently extremely complex: nerf used eight fully-connected layers with 256 nodes each, just to get occupancy, then four more layers of 128 nodes to get the (view-dependent) rgb


one debugging possibility is:
suppose you have K scenes
define K prototypes, sized 256x256x256
make a network that takes the partial view input, and outputs a classification score
we softmax this, and multiply with the prototypes
we should end up with one prototype per scene


after this works, we lower the resolution of the prototypes, add more of them, allow rotations and translations, etc.

in fact, the immediate next step might be to rotate the scenes, and train the detector for this rotation. maybe, actually, rotation could be inferred by cross corr against the true scene, once some selection is done by the model.


instead of having prototypes as free variables, it may in fact be better to have some low-dimensional free variables and a couple deconv layers. but maybe this gets expensive




DONE

figure out the high-level idea

make a model which encodes the scene down to a vector, then uses that vector to soft-select across backgrounds

solve the issue where one template takes over all K scenes
> add noise to logits
>> helps a bit
> div logits by 0.1
>> hurts
> identify this as a exploration/exploitation tradeoff; use epsilon-greedy policy
>> this did not go great
> add some decay to the epsilon in the epsilon-greedy policy
>> skip for now

add feat3dnet, capable maybe of some basic inpainting. let's use RGBO output

add centernet
> feed it the bunch of feats you collected; maybe concat and conv 1x1x1;
> first verify that you can train it with real labels
> worry about rotation later

construct a multiview multi-time dataset, to make life as easy as possible

set up rendernet here, and see what viewpred looks like for this pret model

set up zooming for rendernet

run centernet on each frame and collect its preds


assemble:
> make centernet produce ONE object box, and show that
> declare an object var
> differentiably place the object into the scene, with some kind of bilinear_sample3d
> eliminate the rotation part
> mask the object from featnet and inpaint frame0 with that to get bkg_feat
> use bkg_feat and the placed object to construct an estimate for each timestep
> collect recon loss and see how it goes
>> ok, i have a model that gets OK recon, but it is also producing crazy-looking outputs, with ghost cars and a way-too-big object tensor

add viewpred loss for featnet DISCONNECTED from other stuff. render each full frame and collect loss. (nail the background.) then maybe detach even

add occupancy loss!!!!

make solid scale over time

add center-surround loss: pred boxes should have good center-surround in the forward pass using feat3d features
> actually, the way i did this was not differentiable
> what you can do instead is: for N boxes, get the scores, then create a target tensor and regress to it

swap the detector for a soft argmax variant, so that the model receives more gradients
> set up the viewpred thing here
> still not that great

add multiview loss
> hm, the other views here are not so great



DOING

prove to yourself that you can make the optimization work
> the way in which you find these labels is irrelevant
> later we train models to mimic the labels; distill the knowledge
> so how do you want to do it?
> three var tensors: bkg, obj, spatial position heatmap
>> optimize first for bkg
>>> the truth is, the median bkg is better than this bkg var. but maybe it's good enough
< done
>> do alternating optimization between obj vars and bkg. start with 100 iters of bkg, with no obj, then 100 of obj with bkg fixed, etc, then switch back, etc.
>>> a good way to implement this might be: save the bkg latent after 100 steps, and then work on the object part
>>> for object position, it may make sense to optimize a literal trajectory, using loss against soft argmax peaks and traj smoothness (and of course recon)
>>> try adding a velocity smoothness loss on the softargmax outputs, to see if the gradients are informative enough for that to find something

if you optimize a location var directly, you could collect an encouragement loss from exp(-bilinear_sample(occ_loss, xyz))
it would be nice actually to optimize something that encodes priors about motion. e.g., i could optimize a spline, or clothoid, or that verlet thing
i believe this means optimizing xdotdot (accels). but how do i init?


i believe i need better proposals
for this, let's do bkgsub again, but with these tensors, since they are clean




i think a mistake might be: trying to learn this only for objects. it's too sparse if you do that
instead, try a more convolutional variant, where every single region needs encoding with this
basically this is a capsule net
but maybe the right way to do it is: each capsule is something with a spatial latent and a canonical pose (and even some symmetry)




SOON

after implementing the centroid-only version, implement centroid+scale

resolve the issue of non-differentiable rotation pred
> maybe use soft argmax?
> headings might be important now


LATER

consider storing only half of each protype, and filling in the rest with symmetry
