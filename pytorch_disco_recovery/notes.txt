DONE

fix resample3D

fix the summs to expect ZYX order

get rid of the inconsistency in channel dim position

get featnet working

get occnet working

(adam) install viewnet

swap the 3D bilinear sampler for a cuda version, possibly using grid_sample_3d from pytorch

make the summ_* utils easier to use, by packing in tensor2summ and writer and global_step

replace lines like "if global_step % hyp.log_freq_train == 0" with "if log_this", where log_this is defined at top, depending on global_step and set

fix the issue where a log is created even if we do not make it to first step

fix the issue of summs being weird when B>1

do not log scalars so frequently

add add_loss util, for auto-logging of scaled and unscaled values; start each net with total_loss = 0.0 and add_loss to it

add the safe_l2 from anchors
> resolved via F.normalize

install embnet2D
> replace the existing code with some native pytorch stuff
> check on beta
>> ok it's a const now

install embnet3D

add backwarp for 3d feature learning, for the setup where we train and eval with dense convs

swap the 2D bilinear sampler for a cuda version, possibly using grid_sample_2d from pytorch
> it's indeed noticeably faster

find out if non-logging-iter summs are computed even when not plotted, and if so, fix this
> not happening. but dropped some vis-only computation on non-summ iters

parallelize the voxelizer

add get_random_rt

add 3d augs to front end of featnet (using get_random_rt), to make it unpredictable

add pca option to summ_feat (taking from discovery/anchors)

get rid of prints like "MoviePy - Building file /tmp/tmp2vnknfgb.gif"
> Download this https://github.com/zuoym15/tensorboardX and recomplie tensorboardX.

make singular summ utils

set up and ignore exp_custom.py and custom_go.sh, so that we can experiment locally with fewer conflicts

add model saver and loader

(after custom is done:) set up exp_carla_sta.py to have only standard baseline experiments
> mostly done. exp_carla_sta still needs some cleaning

upgrade summ_feat and summ_feats to be able to do pca with 2D or 3D inputs

get a "valid" mask return out of the resamplers, and use it to avoid penalizing out of bounds regions

add summ_flow and summ_3D_flow to utils_improc

add box/flow preprocessing and computation
> shuffle/sink < done
> get corners < done
> vis < done
> transformer < done 
> on-the-fly scoring < done 
> traj < done
> padded 3d mask < done
> flow < done
> flow-based backwarp util < done

add eye convenience utils

delete the buggy resampler and cuda op (after maybe harvesting code from it)

add cuda corr3d

install a flownet

add time flip aug

parallelize utils_geom.safe_inverse (since we use it a lot now)

set up synth flow supervision

implement sparse conv arch, to save mem

speed up data loading

verify some arch choices for flow
> heatmap size 8 is better than 4, but increases mem consumption a lot
> relu is better than lrelu in the cost volume
> l2 normalization helps

use the vis mask in the emb3d

add min voxel distance constraint for sampling in embnet3D

add min pixel distance constraint for sampling in embnet2D

use the valid mask in the emb2d

add one-sided l2 losses for embnet3D

add sparse-invar convs

add model freezing

add in the careful sparse arch that was figured out in tf

using the synth flow and pret featnet, train a flownet

add higher quality flow synth, by rt'ing the pointcloud itself

add loss that ensures true backwarp with synth_rt has small l2

add preocc net

add a regularization loss to preocc, so that it prefers free

use preocc as sparsity mask in sparse-aware featnet

remove occ_do_cheap (non-)option

evaluate if you can get significantly higher resolution with the sparse convs

get flow loading and init and sup
- precompute flow for your seq of interest < done
- load the flows in np
- get the channels right
- get them in torch
- chain flows into a traj (just translation for now), and vis it
- move the traj into X coords
- use the traj as init
- fix the bug and get the flow that actually corresponds to the frames
- use flow as loss: sample the flow inside the traj, and apply loss=|| flow - (t1-t0) ||
note i am not using the template0 flow, so this has the potential to drift
> also note you cannot compute the template0 flow straightforwardly, since this REQUIRES rigid-backwarping the (single) object

use gt flow as traj loss, just to check < actually worse somehow

add iou 

save/load the optim vars as a separate saverloader func

add flow-based no-drift tracker using tf flow weights
- set up a new mode for this
- load the thing and fire it with random inputs
- fire it true inputs and verify the output
- make big S, and fire the model S-1 times
- add flow-to-rt conversion (inside the box)
- make the flow-to-rt fancier via visibility mask
- push the box using the rt
- add bbox iou computation
- try this with gt flow, to ensure things are ok
- add object-centric rt-backwarp to put the flow back on track
- evaluate iou on the fly
- figure out that you should warp the feats, not the inputs
- figure out that non-obj-centric backwarp works better
- figure out that a shrunk box (in the object interior) is better than a padded box
- save the traj in a way you can load easily for the obj mode
- load the traj in obj mode
- vis the loaded traj
- use the loaded traj as a new init
- save the frame-to-frame flows (not the obj-centric/backwarped stuff)
- load the flows in obj mode
- vis the loaded flows
- use the loaded flows for loss

implement the assembly differently, so that the object gets squished into place
> ok i did this by assembling a halfres object

center-surround of occs as a loss
- no, won't work; it will just mess up the occs

center-surround of FROZEN occs as a loss
- no, won't work; it will just push the object into the air

fix bug in frozen flow, so that you don't make a new sess for every iter

extract proposals from flow
- precompute frame-to-frame flow for the entire dataset, so that you can run through and tune c/s and other heuristics
- set up a mode for tuning c/s
- extract reasonable proposals in bev
- visualize these boxes in perspctive
- add scoring
- add iou/map
- figure out good center-surround thresholds and occ fatness

evaluate flow proposals' mAP

export those models from fangyu and deploy them in pt where they are fast
- port over a clean version of the export code
- export a model
- try that model in pt
- fix the bugs; get it to perform as normal
- export a new model that fangyu provided
- evaluate it on the same task
- verify that the model fangyu says is better indeed performs better (yes)
- get an S=2 forward-backward consistency map and see if you can improve the proposals (no)
- get an S=5 forward-backward consistency map and see if you can improve the proposals (no)
- get S=5 chained flows and see if you can improve the proposals (yes)


add proper mean over 1k evals for flow and proposals
- single step L2 < done
- chained step L2 < done
- single step mAP < done
- multi step mAP < done
- 4 versions of each L2 actually: total, yeszero, nonzero, bal < done
- make si and mu both happen on a single run < done
- evaluate Harley et al. < done. surprisingly strong

add proper object tracking eval

*nearest neighbor eval/vis:*
- hand-select a voxel feature in one frame; 
- generate a heatmap for it in another scene

overfit flow in dome data

Six evaluations:
1) single step flow
2) single step proposals
3) multistep flow
4) multistep flow proposals
5) object tracking
6) nearest neighbor vis

qual and quant SQAIR and Transporter results

intphys data setup

long flow trajectory vis

set up det mode; train a detnet
- set it up supervised 
- vis the results
- add iou/map eval

get 2.5d proposals
> get pwc going
> compute depth
> do egomotion stabilization
> run flownet on the ego-stabilized stuff
> compute proposals from that
> run flownet on the unstabilized stuff, and subtract egoflow
> compute proposals from that

make flow videos from carla data
> feed some val data with noshuf and see if it comes out ok < good

backprop into vars representing the obj and bkg
> set up object mode
> take a pret featnet
> extract the obj and bkg
> declare vars to hold these
> extract the obj and bkg in a step before the main loop
> init the vars with these extractions
> init vars for the trajectories. for now, assume the object never rotates. just optim for the translation.
> visualize the trajectory in a nice bird view
> add a fake loss, just to ensure some optim can happen
> optimize for viewpred
> optimize for 2d and 3d ml
> fill the hole with a sample from another frame
> try a lazy traj init (half of real motion) < done up to here
> try a rough traj ini (1m offset)
> design a loss that uses redetection
> design a loss that uses estimated flow

add zoomed occupancy loss
- strategy1:
-- construct occ at double res, and then sample it with the cropper
--- this is faster and i can do it right now; let's try it and see if it looks bad
--- visually it's ok but i realize now this is a lot of warps. anyway let's see the loss..
- strategy2:
-- construct occ at the correct res/bounds
--- just pass in tlist +- padded_size as the bounds
---- this may in fact be critical for getting solid nonzero occs_g

add trajectory smoothness loss (for angles and translation)

add object-centric view loss (zooming)
- strategy1: 
-- define a small coordinate frame centered on the object
-- for each voxel in that frame, sample from the bkg and sample from the object
-- come up with a small image that represents the zoomed scene there
-- for each pixel in the small image, sample from the zoomed scene
- strategy2:
-- define a small image; for each pixel in there, sample from the vox
--- probably sample from bkg, then sample from obj, and paste one on the other. no.
--- probably assemble a high-res coord frame first, then sample 

note: you may want crops that keep aspect ratio intact, or at least consistent, so that the viewnet does not struggle with this.
> double note: actually there is a secondary issue of zooming greater/smaller distances; this also apparently requires viewnet customization.
>> maybe all these are encouragement for a param-free viewnet

- to get an early experimental result, you can use this focus mode to extract some projections and
-- well, 
--- thing1: train a viewnet with random crops. let's try this first, to find a backprop error or something
--- thing2: set up focused assembly

try several resolutions, supervised, and see if higher res leads to higher map@70 (val)
> running now
>> discarded; not much effect after 50k or so

set up det for our self-sup setting, in which we have one labelset for training and another set for eval

use the oracle proposals as labels, and train a detnet to mimic


DOING

think hard about handling lopsided objects

think hard about 3d det via seg/grouping
> ask allie for her paper
>> wait for result


add https://github.com/zuoym15/tensorboardX to readme





SOON

do sparse-aware convs using occ+free as the mask, rather than just occ. 

track kitti cars, using frozen features and flow weights

try replacing the last upconv with upsampling

add voxelnet-style augs

swap flow mag conn comp for some fancier clustering, so that i can discover cars that move in opposite directions 

try again that new-flow optim; i think there was a bug in my last try, where i expoted R0 flow, but really i need RX

cycle loss at the box level
- what does this mean exactly?
-- track forward and backward and ensure you end up in the same place.
-- i think a prerequisite of this is to compute backward flow. i can do that easily enough.

make the zoom dimensions square and unrelated to the original dims

add crisp crop of zoom region from original rgb

wrap up boxlist2d computation

add 2d box agreement loss

copy in that middle net from the spconv repo, and see if it improves somthing

make smart sparse-aware featnet at high res

evaluate if the sparse-conv net gets better val perf than the dense one

evaluate if the param-free renderer gets better val perf than the learned one

try a fancier clustering method, to manage multidirectional things better

write data with the hand inside

find out why crop_and_resize and obj projection have a tiny spatial mismatch
> i believe this is because grid_sample right now has align_corners=False (with no option to make it true); this is fixed in pytorch 1.3

see if preocc net needs to use an alt view as input

address the weakness where: a large bbox with the object shoved in the corner may get equivalent loss to a snug box

consider giving the bkg feat bigger metric space than what we use for the per-frame feats, so that it can explain more

check flow against the tf version; i think performance was better there

try a do_free version of flownet, to ensure the incremental warp and heatmaps are well-formulated

evaluate one one-sided loss vs all

load from ckpt and train flow on top

pretrain a nice featnet
> we want 4 versions i think:
>> standard < done
>> sparse-invar convs < done
>> sparse convs
>> sparse sparse-invar convs

avoid warping the first guy when warping X0_T_Xs

debug and add the retrieval eval

add sparsity-invariant convs, using the /adaptive_denom method (implemented but uncommitted in discovery)

add utils_geom.apply_4x4s convenience util (plural wrapper for utils_geom.apply_4x4)

eliminate "from utils_basic import *" and instead import utils_basic and call it clearly
> utils_geom: done
> utils_misc: done

rename "ref" to simply "cam" everywhere


LATER

do a just-in-time mkdir for logging, to avoid the current cluttering issue

debug the colorize util, so that we can get something prettier than grayscale

add one-sided L2 losses for embnet3D




