DONE
make a carla_entity mode, with multiview inputs
give _entity mode some optimizable vars and get it working for viewpred

train zi directly, at the locations where we have rgb

optimize for depth/shape pred, instead of rgb, to avoid the textures/shadows entirely
> some strange artifacts
> i believe these would go away with more viewpoints

make a background var, shared across all scenes, to see what happens
> ok it looks like junk

train a basic feat3d capable of producing view renders here
> make this in a new mode, maybe _render

get a base feat3d going
> load a pret feat3dnet capable of rendering
> eliminate objects from its input
>> get object masks
>> mask out


DOING

get dynamic-resolution occ/feat, by querying at subvoxel locations and resolving with an MLP
> start with a single-scale net
> start with just occupancy upscaling, before also predicting rgb
> add random uniform sampling


i think our autoregressive ideas actually come into play here
> what i want is to condition on a subset of points, and generate new points
> and i want to generate at the frontier
> but what i do not want to do is:
>> generate when the answer could be more easily acquired with bilinear interpolation
>> compute answers densely when i am only going to use them sparsely
so, what i need is:
> sparse autoregressive architecture
> train it for binary generation
> the net should not be too complex. small FOV probably, like 9x9x9
>> no, that's wrong, you need wide fov to know that big objects end
> you need coarse-to-fine
> small FOV is OK, but generate coarse first, and always condition on the coarser levels
> at the coarsest level we should have neurons that see almost the whole scene
so, the strategy is:
> at each scale, starting from 1/16 maybe, condition on
>> (1) the freespace and pointcloud voxelized at this resolution
>> (2) the coarser-scale outputs trilinearly upsampled
> and we ask this model to compute answers at the *unknown part of the* frontier
> since the model only has a 9x9x9 fov, we only need to: dilate once, and use that as the computation mask for subm convs
> it's probably ok to sample for the entire frontier simultaneously (not conditioned on each other)
>> but that's a detail to work out empirically






make a kpconv net, so that you can more naturally query at continuous locations, and not waste time with 3d convs




SOON

add a perceptual loss

make a background var, and some object vars

learn a detector that can place the objects into the background

randomize the projection matrix a bit, so that you penalize a variety of subpixel rays


LATER

